{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d18a343-c53e-4e64-8316-f5c1167437c4",
   "metadata": {},
   "source": [
    "# Laboratorium 6 - rekomendacje oparte na grafach wiedzy\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge\n",
    "   * na potrzeby drugiej części laboratorium (czyli testowego treningu), na Teamsach macie dostępny podzbiór danych, `a_few_playlists_dataset` - nie wystarczy on jednak do wykonania trzeciej części (i tym samym do oddania laboratorium)\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab6`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install numpy pandas pykeen tqdm seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a763f-eb56-4085-ac43-6d7a1a0cb520",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e62e94d-9961-4d5e-a264-745b5177ed5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:22:02.130293Z",
     "start_time": "2023-12-04T19:22:02.119782500Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from pykeen.models import TransE\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.triples import TriplesFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67368f56-c5ee-45cb-90f7-235d610c40dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:22:04.342107300Z",
     "start_time": "2023-12-04T19:22:04.324028600Z"
    }
   },
   "outputs": [],
   "source": [
    "# konfiguracja datasetu Spotify\n",
    "PATH = 'spotify_million_playlist_dataset/data'\n",
    "SAMPLING_RATIO = 1.0\n",
    "FILENAMES = random.sample([f'mpd.slice.{1000*i}-{1000*i+999}.json' for i in range(1000)], int(SAMPLING_RATIO*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d3db7-5e55-4a11-a45e-79356fd75e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jesli uzywasz datasetu pobranego z Teamsow, uzyj tej komorki zamiast powyzszej\n",
    "# UWAGA - do oddania laboratorium konieczne jest uzycie oficjalnego datasetu Spotify\n",
    "# PATH = 'a_few_playlists_dataset'\n",
    "# SAMPLING_RATIO = 0.01\n",
    "# with open(f'{PATH}/filenames.txt') as fn:\n",
    "#     FILENAMES = fn.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7743626d-5017-4443-809c-f3146bf82298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:22:11.104725500Z",
     "start_time": "2023-12-04T19:22:11.078147700Z"
    }
   },
   "outputs": [],
   "source": [
    "# funkcje do parsowania playlist\n",
    "def get_id(uri):\n",
    "    return uri.split(':')[-1]\n",
    "\n",
    "def parse_playlist(playlist):\n",
    "    name = playlist['name']\n",
    "    tracks = [get_id(t['track_uri']) for t in playlist['tracks']]\n",
    "    tracks_to_artists = {(get_id(t['track_uri']), get_id(t['artist_uri'])) for t in playlist['tracks']}\n",
    "    tracks_to_albums = {(get_id(t['track_uri']), get_id(t['album_uri'])) for t in playlist['tracks']}\n",
    "    albums_to_artists = {(get_id(t['album_uri']), get_id(t['artist_uri'])) for t in playlist['tracks']}\n",
    "    return name, tracks, tracks_to_artists, tracks_to_albums, albums_to_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475e2848-3c30-4173-98dc-272fb86e4690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:30:56.443203700Z",
     "start_time": "2023-12-04T19:22:13.292327900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:40<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: 1,000,000 playlists; 2,262,292 tracks; 734,684 albums; 295,860 artists\n"
     ]
    }
   ],
   "source": [
    "# parsing\n",
    "playlists = []\n",
    "tracks = set()\n",
    "tracks_to_artists = set()\n",
    "tracks_to_albums = set()\n",
    "albums_to_artists = set()\n",
    "\n",
    "for filename in tqdm(FILENAMES):\n",
    "    with open(f'{PATH}/{filename}') as mpd_chunk:\n",
    "        for playlist in json.loads(mpd_chunk.read())['playlists']:\n",
    "            a, b, c, d, e = parse_playlist(playlist)\n",
    "            playlists.append(b)\n",
    "            tracks.update(set(b))\n",
    "            tracks_to_artists.update(c)\n",
    "            tracks_to_albums.update(d)\n",
    "            albums_to_artists.update(e)\n",
    "\n",
    "print(f'Got: {len(playlists):,} playlists; {len(tracks):,} tracks; {len({x[1] for x in tracks_to_albums}):,} albums; {len({x[1] for x in tracks_to_artists}):,} artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a6478c-9602-4e40-ae2f-8eeea8a665ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:33:34.183169800Z",
     "start_time": "2023-12-04T19:32:21.143443500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:39<00:00, 25169.55it/s]\n",
      "100%|██████████| 1000000/1000000 [00:32<00:00, 30696.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 900,000; test dataset: 100,000\n"
     ]
    }
   ],
   "source": [
    "# w zbiorze testowym chcemy tylko te playlisty, ktorych wszystkie piosenki wystapia takze choc raz w zbiorze treningowym\n",
    "tracks_counter = {}\n",
    "for p in tqdm(playlists):\n",
    "    for t in p:\n",
    "        if t in tracks_counter:\n",
    "            tracks_counter[t] += 1\n",
    "        else:\n",
    "            tracks_counter[t] = 1\n",
    "\n",
    "playlists_with_only_non_unique_tracks = []\n",
    "for i in tqdm(range(len((playlists)))):\n",
    "    p = playlists[i]\n",
    "    if all([tracks_counter[t] > 1 for t in p]):\n",
    "        playlists_with_only_non_unique_tracks.append(i)\n",
    "\n",
    "# zbior testowy to 1/10 wszystkich playlist - czyli 100k, jesli nie używamy samplingu\n",
    "test_playlist_ids = random.sample(playlists_with_only_non_unique_tracks, int(SAMPLING_RATIO*100_000))\n",
    "# zbior treningowy to cala reszta playlist - jest ich duzo, wiec sprobujmy to zrobic wydajnie\n",
    "test_ids_sorted = sorted(test_playlist_ids)\n",
    "test_i = 0\n",
    "train_playlist_ids = []\n",
    "i = 0\n",
    "while i < len(playlists):\n",
    "    if test_i < len(test_ids_sorted) and test_ids_sorted[test_i] == i:\n",
    "        test_i += 1\n",
    "    else:\n",
    "        train_playlist_ids.append(i)\n",
    "    i += 1\n",
    "\n",
    "train_playlists = [playlists[i] for i in train_playlist_ids]\n",
    "test_playlists = [playlists[i] for i in test_playlist_ids]\n",
    "\n",
    "print(f'train dataset: {len(train_playlists):,}; test dataset: {len(test_playlists):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480cce3-90df-4f6e-b781-0c30f6c18c31",
   "metadata": {},
   "source": [
    "## Część 2. - budowa i ewaluacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bb789-088e-4d5b-82ac-15116fb31216",
   "metadata": {},
   "source": [
    "### Relacje istniejące w naszym datasecie:\n",
    "![poglądowy obrazek relacji w datasecie](relations.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83962cc3-54fd-438d-a7c7-b8a2f3c1b4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T19:40:21.237759400Z",
     "start_time": "2023-12-04T19:39:42.353578600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2262292/2262292 [00:02<00:00, 1119283.50it/s]\n",
      "100%|██████████| 2262292/2262292 [00:01<00:00, 1223267.65it/s]\n",
      "100%|██████████| 849766/849766 [00:00<00:00, 1263397.92it/s]\n",
      "100%|██████████| 900000/900000 [00:34<00:00, 26252.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 4 relations with total of 64,805,959 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# budowanie zbioru relacji\n",
    "# mozesz usunac czesc wpisow z listy `relations`\n",
    "relations = ['follows', 'authored_by', 'in_album', 'authored']\n",
    "triples = []\n",
    "\n",
    "# relacje piosenka -> autor\n",
    "if 'authored_by' in relations:\n",
    "    for track, artist in tqdm(tracks_to_artists):\n",
    "        triples.append((track, 'authored_by', artist))\n",
    "\n",
    "# relacje piosenka -> artysta\n",
    "if 'in_album' in relations:\n",
    "    for track, album in tqdm(tracks_to_albums):\n",
    "        triples.append((track, 'in_album', album))\n",
    "\n",
    "# relacje artysta -> album\n",
    "if 'authored' in relations:\n",
    "    for album, artist in tqdm(albums_to_artists):\n",
    "        triples.append((artist, 'authored', album))\n",
    "\n",
    "# relacje piosenka -> piosenka\n",
    "if 'follows' in relations:\n",
    "    for playlist in tqdm(train_playlists):\n",
    "        for i in range(len(playlist)-1):\n",
    "            triples.append((playlist[i], 'follows', playlist[i+1]))\n",
    "\n",
    "num_entities = len(triples)\n",
    "num_relations = len(relations)\n",
    "\n",
    "print(f'Got {num_relations} relations with total of {num_entities:,} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a2042a-3165-4177-af82-2cb1586c8eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T20:13:00.327857Z",
     "start_time": "2023-12-04T19:41:38.866262400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=3873791339\n",
      "No random seed is specified. Setting to 271162122.\n",
      "No cuda devices were available. The model runs on CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training epochs on cpu:   0%|          | 0/1 [00:00<?, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "742e1a64553f4688bdcba092c36f04a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training batches on cpu:   0%|          | 0/151476 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc3b84e7392c4fb1954ae682a7203cf1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# ta dysproporcja jest po to, by szybko uzyskac jakikolwiek wynik\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#   - dla uzyskania sensownych wynikow warto zmienic split na np. standardowe 80-10-10\u001B[39;00m\n\u001B[0;32m      5\u001B[0m training, testing, validation \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msplit([\u001B[38;5;241m.9899\u001B[39m, \u001B[38;5;241m.01\u001B[39m, \u001B[38;5;241m.0001\u001B[39m])\n\u001B[1;32m----> 7\u001B[0m pipeline_result \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtesting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtesting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTransE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# to najszybszy i najprostszy, ale i najgorszy model; pomysl o uzyciu TransH, TransR, RESCAL albo dowolnego innego\u001B[39;49;00m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# to zdecydowanie za malo - 1 wystarczy do jakichkolwiek wynikow, 5 do dosc slabych, blizej 20 do sensownych\u001B[39;49;00m\n\u001B[0;32m     13\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pykeen\\pipeline\\api.py:1546\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001B[0m\n\u001B[0;32m   1525\u001B[0m training_loop_instance \u001B[38;5;241m=\u001B[39m _handle_training_loop(\n\u001B[0;32m   1526\u001B[0m     _result_tracker\u001B[38;5;241m=\u001B[39m_result_tracker,\n\u001B[0;32m   1527\u001B[0m     model_instance\u001B[38;5;241m=\u001B[39mmodel_instance,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1536\u001B[0m     negative_sampler_kwargs\u001B[38;5;241m=\u001B[39mnegative_sampler_kwargs,\n\u001B[0;32m   1537\u001B[0m )\n\u001B[0;32m   1539\u001B[0m evaluator_instance, evaluation_kwargs \u001B[38;5;241m=\u001B[39m _handle_evaluator(\n\u001B[0;32m   1540\u001B[0m     _result_tracker\u001B[38;5;241m=\u001B[39m_result_tracker,\n\u001B[0;32m   1541\u001B[0m     evaluator\u001B[38;5;241m=\u001B[39mevaluator,\n\u001B[0;32m   1542\u001B[0m     evaluator_kwargs\u001B[38;5;241m=\u001B[39mevaluator_kwargs,\n\u001B[0;32m   1543\u001B[0m     evaluation_kwargs\u001B[38;5;241m=\u001B[39mevaluation_kwargs,\n\u001B[0;32m   1544\u001B[0m )\n\u001B[1;32m-> 1546\u001B[0m stopper_instance, configuration, losses, train_seconds \u001B[38;5;241m=\u001B[39m \u001B[43m_handle_training\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1547\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_result_tracker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_result_tracker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1548\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1550\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_instance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1551\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluator_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluator_instance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_loop_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_loop_instance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclear_optimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclear_optimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluation_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1557\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstopper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1558\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstopper_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopper_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_tqdm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1562\u001B[0m metric_results, evaluate_seconds \u001B[38;5;241m=\u001B[39m _handle_evaluation(\n\u001B[0;32m   1563\u001B[0m     _result_tracker\u001B[38;5;241m=\u001B[39m_result_tracker,\n\u001B[0;32m   1564\u001B[0m     model_instance\u001B[38;5;241m=\u001B[39mmodel_instance,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1575\u001B[0m     use_tqdm\u001B[38;5;241m=\u001B[39muse_tqdm,\n\u001B[0;32m   1576\u001B[0m )\n\u001B[0;32m   1577\u001B[0m _result_tracker\u001B[38;5;241m.\u001B[39mend_run()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pykeen\\pipeline\\api.py:1190\u001B[0m, in \u001B[0;36m_handle_training\u001B[1;34m(_result_tracker, training, validation, model_instance, evaluator_instance, training_loop_instance, clear_optimizer, evaluation_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, use_tqdm)\u001B[0m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;66;03m# Train like Cristiano Ronaldo\u001B[39;00m\n\u001B[0;32m   1189\u001B[0m training_start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m-> 1190\u001B[0m losses \u001B[38;5;241m=\u001B[39m training_loop_instance\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m   1191\u001B[0m     triples_factory\u001B[38;5;241m=\u001B[39mtraining,\n\u001B[0;32m   1192\u001B[0m     stopper\u001B[38;5;241m=\u001B[39mstopper_instance,\n\u001B[0;32m   1193\u001B[0m     clear_optimizer\u001B[38;5;241m=\u001B[39mclear_optimizer,\n\u001B[0;32m   1194\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtraining_kwargs,\n\u001B[0;32m   1195\u001B[0m )\n\u001B[0;32m   1196\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m losses \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# losses is only none if it's doing search mode\u001B[39;00m\n\u001B[0;32m   1197\u001B[0m training_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m training_start_time\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pykeen\\training\\training_loop.py:378\u001B[0m, in \u001B[0;36mTrainingLoop.train\u001B[1;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, clear_optimizer, checkpoint_directory, checkpoint_name, checkpoint_frequency, checkpoint_on_failure, drop_last, callbacks, callback_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001B[0m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;66;03m# send model to device before going into the internal training loop\u001B[39;00m\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mto(get_preferred_device(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, allow_ambiguity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m--> 378\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mslice_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslice_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43msampler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontinue_training\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontinue_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_size_probing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_size_probing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_tqdm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_tqdm_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_tqdm_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtqdm_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtqdm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m        \u001B[49m\u001B[43msub_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msub_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_checkpoints\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_checkpoints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    393\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    394\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_frequency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_frequency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_on_failure_file_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_on_failure_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    396\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbest_epoch_model_file_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbest_epoch_model_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlast_best_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlast_best_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    401\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_clipping_max_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_clipping_max_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_clipping_norm_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_clipping_norm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_clipping_max_abs_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_clipping_max_abs_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtriples_factory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtriples_factory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpin_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpin_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    406\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# Ensure the release of memory\u001B[39;00m\n\u001B[0;32m    409\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pykeen\\training\\training_loop.py:660\u001B[0m, in \u001B[0;36mTrainingLoop._train\u001B[1;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks, callback_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001B[0m\n\u001B[0;32m    657\u001B[0m     callback\u001B[38;5;241m.\u001B[39mpre_step()\n\u001B[0;32m    659\u001B[0m     \u001B[38;5;66;03m# update parameters according to optimizer\u001B[39;00m\n\u001B[1;32m--> 660\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[38;5;66;03m# After changing applying the gradients to the embeddings, the model is notified that the forward\u001B[39;00m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;66;03m# constraints are no longer applied\u001B[39;00m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpost_parameter_update()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    371\u001B[0m             )\n\u001B[1;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:163\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    152\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    155\u001B[0m         group,\n\u001B[0;32m    156\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    160\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    161\u001B[0m         state_steps)\n\u001B[1;32m--> 163\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:311\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 311\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:384\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    381\u001B[0m     param \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(param)\n\u001B[0;32m    383\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m--> 384\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlerp_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# trening\n",
    "tf = TriplesFactory.from_labeled_triples(np.array(triples))\n",
    "# ta dysproporcja jest po to, by szybko uzyskac jakikolwiek wynik\n",
    "#   - dla uzyskania sensownych wynikow warto zmienic split na np. standardowe 80-10-10\n",
    "training, testing, validation = tf.split([.9899, .01, .0001])\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    training=training,\n",
    "    testing=testing,\n",
    "    validation=validation,\n",
    "    model=TransE, # to najszybszy i najprostszy, ale i najgorszy model; pomysl o uzyciu TransH, TransR, RESCAL albo dowolnego innego\n",
    "    epochs=1 # to zdecydowanie za malo - 1 wystarczy do jakichkolwiek wynikow, 5 do dosc slabych, blizej 20 do sensownych\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'<table>\\n<thead>\\n<tr><th>Key            </th><th>Value                   </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>OS             </td><td>nt                      </td></tr>\\n<tr><td>Platform       </td><td>Windows                 </td></tr>\\n<tr><td>Release        </td><td>10                      </td></tr>\\n<tr><td>Time           </td><td>Mon Dec  4 21:24:06 2023</td></tr>\\n<tr><td>Python         </td><td>3.9.12                  </td></tr>\\n<tr><td>PyKEEN         </td><td>1.10.1                  </td></tr>\\n<tr><td>PyKEEN Hash    </td><td>UNHASHED                </td></tr>\\n<tr><td>PyKEEN Branch  </td><td>                        </td></tr>\\n<tr><td>PyTorch        </td><td>2.1.1+cpu               </td></tr>\\n<tr><td>CUDA Available?</td><td>false                   </td></tr>\\n<tr><td>CUDA Version   </td><td>N/A                     </td></tr>\\n<tr><td>cuDNN Version  </td><td>N/A                     </td></tr>\\n</tbody>\\n</table>'",
      "text/html": "<table>\n<thead>\n<tr><th>Key            </th><th>Value                   </th></tr>\n</thead>\n<tbody>\n<tr><td>OS             </td><td>nt                      </td></tr>\n<tr><td>Platform       </td><td>Windows                 </td></tr>\n<tr><td>Release        </td><td>10                      </td></tr>\n<tr><td>Time           </td><td>Mon Dec  4 21:24:06 2023</td></tr>\n<tr><td>Python         </td><td>3.9.12                  </td></tr>\n<tr><td>PyKEEN         </td><td>1.10.1                  </td></tr>\n<tr><td>PyKEEN Hash    </td><td>UNHASHED                </td></tr>\n<tr><td>PyKEEN Branch  </td><td>                        </td></tr>\n<tr><td>PyTorch        </td><td>2.1.1+cpu               </td></tr>\n<tr><td>CUDA Available?</td><td>false                   </td></tr>\n<tr><td>CUDA Version   </td><td>N/A                     </td></tr>\n<tr><td>cuDNN Version  </td><td>N/A                     </td></tr>\n</tbody>\n</table>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pykeen\n",
    "pykeen.env()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T20:24:06.061327600Z",
     "start_time": "2023-12-04T20:24:06.044554400Z"
    }
   },
   "id": "41f395e74e3c83ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bf67a-f449-4296-91e5-cadad5d4990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zgrubne oszacowanie jakosci wytrenowanego modelu\n",
    "pipeline_result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eaf5f3-8083-4284-8156-4f8326671050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja zwracajaca predykcje z modelu\n",
    "def predict_next_tracks(track_id: str, k: int) -> list[str]:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83eba4-2e1e-4bec-be87-8063ed71ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metryki do porownania rekomenderow - precision@k i recall@k\n",
    "def precision(prediction: list[str], actual_tracks: list[str]) -> float:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def recallprediction: list[str], actual_tracks: list[str]) -> float:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cbf2e-e686-45d0-a8cd-5343132b2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocena wynikow\n",
    "\n",
    "# odsiewamy playlisty zbyt krotkie, by dac sensowne wyniki\n",
    "long_enough_test_playlists = [p for p in test_playlists if len(p) >= 10]\n",
    "# z kazdej playlisty, elementy od 0 do `cutoff_idx` wlacznie sa dane, na ich podstawie robimy predykcje\n",
    "# elementy od cutoff_idx+1 do konca powinnismy umiec przewidziec\n",
    "cutoff_idx = 4\n",
    "# ile elementow ma przewidziec nasz model\n",
    "k = 20\n",
    "precisions: list[float] = # ...\n",
    "recalls: list[float] = # ...\n",
    "\n",
    "# histogram z wynikami\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(precisions, bins=20)\n",
    "ax1.set_title('Precision')\n",
    "ax2.hist(recalls, bins=20)\n",
    "ax2.set_title('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d83b9-de72-4fc0-8bfe-7acb6c66f62f",
   "metadata": {},
   "source": [
    "## Część 3. - porównanie różnych metod rekomendacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87e7c-b9fb-430d-86cb-14433ddd4287",
   "metadata": {},
   "source": [
    "W części 2. zbudowaliśmy zbiór trójek relacji, wytrenowaliśmy jeden model, zaimplementowaliśmy (prostą) metodę generującą rekomendacje na podstawie predykcji modelu i w końcu zaimplementowaliśmy dwie metryki do porównania jakości tych rekomendacji.\n",
    "\n",
    "W części 3. Twoim zadaniem jest przetestować trzy różne podejścia do jednego z kroków:\n",
    "1. Porównaj trzy różne modele spośród dostępnych w bibliotece PyKeen: https://pykeen.readthedocs.io/en/stable/reference/models.html#classes\n",
    "   * jeden model translacyjny (np. TransE, TransH, TransR)\n",
    "   * jeden model faktoryzacyjny (np. RESCAL)\n",
    "   * jeden dowolny model niewybrany w poprzednich punktach\n",
    "2. Porównaj trzy metody budowania grafu wiedzy:\n",
    "   * graf zawierający relacje wszystkich czterech typów\n",
    "   * graf zawierający tylko relacje typu `follows` (czyli między kolejnymi utworami w playliście)\n",
    "   * graf zawierający relacje wybranych przez Ciebie dwóch lub trzech typów (czyli krok pośredni między powyższymi punktami)\n",
    "3. Porównaj trzy metody generowania rekomendacji na podstawie elementów zwróconych przez `predict_target()` (ta metoda zwraca m. in. score'y każdego z proponowanych elementów, co może okazać się pomocne):\n",
    "   * metoda opierająca się tylko na predykcji dla ostatniego znanego elementu w playliście\n",
    "   * dwie wymyślone przez Ciebie, bardziej zaawansowane metody\n",
    "  \n",
    "Niezależnie od tego, który z trzech powyższych scenariuszy wybierzesz - porównaj trzy wybrane przez Ciebie metody na podstawie histogramów metryk `precision@k` i `recall@k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e81534-66a5-4a81-a974-90b6496dffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miejsce na Twoją implementację\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
