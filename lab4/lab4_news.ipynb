{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4 - rekomendacje dla portali informacyjnych\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * więcej możesz poczytać tutaj: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy wszystkie potrzebne pakiety\n",
    "\n",
    "import codecs\n",
    "from collections import defaultdict # mozesz uzyc zamiast zwyklego slownika, rozwaz wplyw na czas obliczen\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# mozesz uzyc do obliczania najbardziej podobnych tekstow zamiast liczenia \"na piechote\"\n",
    "# ale pamietaj o dostosowaniu formatu danych\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiujemy potrzebne zmienne\n",
    "\n",
    "PATH = './MINDsmall_train'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytujemy metadane artykułów\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizujemy teksty na potrzeby dalszego przetwarzania\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # zamieniamy wszystkie ciagi bialych znakow na pojedyncze spacje\n",
    "    # usuwamy znaki interpunkcyjne\n",
    "    # usuwamy wszystkie liczby\n",
    "    # podmieniamy wszystkie wielkie litery\n",
    "    # dzielimy na tokeny\n",
    "    # usuwamy stopwords\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def stem_texts(corpus):\n",
    "    stemmer = # przetestuj rozne stemmery\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porownajmy teksty przed i po przetworzeniu\n",
    "\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy liste wszystkich slow w korpusie\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    # generujemy posortowana alfabetycznie liste wszystkich slow (tokenow)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe tekstow, w ktorych wystapilo kazde ze slow\n",
    "# pamietaj, ze jesli slowo wystapilo w danym tekscie wielokrotnie, to liczymy je tylko raz\n",
    "\n",
    "def get_document_frequencies(corpus, wordlist):\n",
    "    # return {word -> count}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    # return {news_id -> {word -> count}}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy metryke tf_idf\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    raise NotImplementedError()\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - Podobieństwo tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczmy odleglosc miedzy dwoma artykulami\n",
    "# przetestuj rozne metryki odleglosci i wybierz najlepsza\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[2], news_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wyznaczmy k najpodobniejszych tekstow do danego\n",
    "# pamietaj o odpowiedniej kolejnosci sortowania w zaleznosci od wykorzystanej metryki\n",
    "# pamietaj, zeby wsrod podobnych tekstow nie bylo danego\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus, news_indices):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k)\n",
    "    print(f'id: {n_id}, text: {corpus[news_indices[n_id]]}')\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "        print(f'\\nid: {s_id}, text: {corpus[news_indices[s_id]]}')\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[42337], 5, texts, news_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
